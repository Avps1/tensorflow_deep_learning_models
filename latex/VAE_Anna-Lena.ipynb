{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproduction\n",
    "np.random.seed(seed=0)\n",
    "tf.set_random_seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import the MNIST Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1):\n",
    "    \"\"\"\n",
    "    Xavier initialization of the weights. \n",
    "    @param fan_in: umber of incoming connections\n",
    "    @param fan_out: number of outgoing connections\n",
    "\n",
    "    output: a tensor of shape (fan_in, fan_out) filled with random uniform values\n",
    "    \"\"\"\n",
    "    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    high = constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    return tf.Variable(tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, net_architecture, activation_fct=tf.nn.softplus, learning_rate=.001, batch_size=100):\n",
    "        \n",
    "        # Network parameters\n",
    "        self.net_architecture = net_architecture\n",
    "        self.activation_fct = activation_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Network input\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, network_architecture[\"n_input\"]])\n",
    "        \n",
    "    def _create_net(self):\n",
    "        \n",
    "        # Initialize all weights and biases of the VAE\n",
    "        net_weights = self._init_weights(**self.net_architecture)\n",
    "\n",
    "        # Use the encoder network to compute the mean and variance of a Gaussian distribution \n",
    "        # From this we can sample a value of z\n",
    "        # z = mu + sigma*epsilon\n",
    "        \n",
    "        eps = tf.random_normal((sef.batch_size, n_z))\n",
    "        \n",
    "    def _init_weights(self, n_input, n_z, h1_encoder, h2_encoder, h1_decoder, h2_decoder,):\n",
    "        \"\"\"\n",
    "        Initializes all network weights using the Xavier initialization.\n",
    "        All network weights are stored in dictionaries, summarized in a large dictionary\n",
    "        \n",
    "        @param n_input: dimensionality of input data\n",
    "        @param n_z: dimensionality of latent space\n",
    "        \n",
    "        returns: dictionary of dictionaries\n",
    "        \"\"\"\n",
    "        \n",
    "        all_weights = {}\n",
    "        \n",
    "        all_weights['weights_encoder'] = {\n",
    "            'h1': xavier_init(n_input, h1_encoder),\n",
    "            'h2': xavier_init(h1_encoder, h2_encoder),\n",
    "            'means': xavier_init(h2_encoder, n_z),\n",
    "            'sigmas': xavier_init(h2_encoder, n_z)\n",
    "        }\n",
    "        \n",
    "        all_weights['biases_encoder'] = {\n",
    "            'b1': tf.Variable(tf.zeros(shape=[h1_encoder], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros(shape=[h2_encoder], dtype=tf.float32)),\n",
    "            'means': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'sigmas': tf.Variable(tf.zeros([n_z], dtype=tf.float32))\n",
    "        }\n",
    "        \n",
    "        all_weights['weights_decoder'] = {\n",
    "            'h1': xavier_init(n_z, h1_decoder),\n",
    "            'h2': xavier_init(h1_decoder, h2_decoder),\n",
    "            'means': xavier_init(h2_decoder, n_input),\n",
    "            'sigmas': xavier_init(h2_decoder, n_input)\n",
    "        }\n",
    "        \n",
    "        all_weights['biases_decoder'] = {\n",
    "            'b1': tf.Variable(tf.zeros(shape=[h1_decoder], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros(shape=[h2_decoder], dtype=tf.float32)),\n",
    "            'means': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'sigmas': tf.Variable(tf.zeros([n_input], dtype=tf.float32))  \n",
    "        }\n",
    "        \n",
    "        return all_weights\n",
    "    \n",
    "    \n",
    "    def _encoder_network(self, weights, biases):\n",
    "        \"\"\"\n",
    "        The encoder network is used to approximate the true posterior p(z|x)\n",
    "        using the variational distribution q_phi(z|x). Usually, q_phi(z|x) is\n",
    "        taken to be a Gaussian distribution with a diagonal covariance matrix\n",
    "        whose mean and variance vectors are parametrized by a neural network\n",
    "        with input x.\n",
    "        So our encoder network takes x as an input and produces a vector of \n",
    "        means and a vector of variances of a Gaussian distrbution. From this\n",
    "        we can sample values of z, i.e. z ~ q_phi(z|x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # The output of each layer is given by applying the activation function\n",
    "        # to a linear combination of the input and weights\n",
    "        layer1 = self.activation_fct(tf.add(tf.matmul(self.x, weights['h1']), biases['b1']))\n",
    "        layer2 = self.activation_fct(tf.add(tf.matmul(layer1, weights['h2']), biases['b2']))\n",
    "        \n",
    "        # The vector of means and standard deviations is computed in a similar\n",
    "        # fashion but without applying an activation function\n",
    "        means = tf.add(tf.matmul(layer2, weights['means']), biases['means'])\n",
    "        log_sigmas = tf.add(tf.matmul(layer2, weights['sigmas']), biases['sigmas'])\n",
    "\n",
    "        # As mentioned above, the network outputs a vector of means and a\n",
    "        # vector of variances\n",
    "        return(means, log_sigmas)\n",
    "        \n",
    "  \n",
    "    def _decoder_network(self, weights, biases):\n",
    "        \"\"\"\n",
    "        The decoder network takes a latent variable z as an input and reproduces\n",
    "        the input x. In our case, it maps z onto a Bernoulli distrbution.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Again, the output of each layer is given by applying the activation \n",
    "        # function to a linear combination of the input and weights\n",
    "        layer1 = self.activation_fct(tf.add(tf.matmul(self.z, weights['h1']), biases['h1']))\n",
    "        layer2 = self.activation_fct(tf.add(tf.matmul(layer1, weights['h2']), biases['h2']))\n",
    "        \n",
    "        # Wieso reicht es hier aus, nur einen mean zu berechnen?\n",
    "        # Berechne ich ueberhaupt einen mean? oder was ist hier der output?\n",
    "        means_x = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weights['means']), biases['means']))\n",
    "        \n",
    "        # Was ist hier der output? Wieso nur ein output?\n",
    "        return means_x\n",
    "        \n",
    "        \n",
    "    def _loss_optimizer(self):\n",
    "        \"\"\"\n",
    "        The loss function has two terms\n",
    "        \n",
    "        1) The reconstruction loss: -log(p(x|z))\n",
    "        \n",
    "        2) The latent loss: Kullback-Leibler divergence between q_phi(z|x) and the prior\n",
    "        p(z). This loss acts\n",
    "        like a regularizer.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstr_loss = -tf.reduce_sum(self.x * )\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
