{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNN in TensorFlow\n",
    "\n",
    "This notebook explains how to build a simple recurrent neural network (RNN) in TensorFlow. It does *not* use the TensorFlow RNN API but performs all computations manually (which helps to understand what is happening in an RNN). The code is based on an existing [tutorial](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation: truncated Backpropagation\n",
    "\n",
    "A recurrent neural network is designed in a way such that the output at a certain time step depends on arbitrarily distant inputs. In other words: when building an RNN in TensorFlow, the graph would have to be as wide as the input sequence.\n",
    "Unfortunately, this makes backpropagation computation both expensive and ineffective because gradients propagated over many time steps tend to either vanish (most of the time) or explode.\n",
    "\n",
    "A common solution to this problem is to create an \"unrolled\" version of the recurrent network that contains a fixed number (*truncated_backprop_steps*) of RNN inputs and outputs. In other words: backpropagation is \"truncated\" such that errors are only backpropagated for a fixed number of steps. A higher number of steps enables capturing long-term dependencies but is also more expensive (both regarding memory and computation).\n",
    "\n",
    "The model is then trained on this finite approximation of the recurrent network. Accordingly, at each time step the network is fed with inputs of length *truncated_backprop_steps*. The backward pass is performed after each input block. A short explanation is given on TensorFlow's [website](https://www.tensorflow.org/tutorials/recurrent#truncated-backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global configuration parameters\n",
    "n_epochs = 20\n",
    "total_series_length = 50000\n",
    "truncated_backprop_steps = 15\n",
    "state_size = 4 \n",
    "n_classes = 2\n",
    "echo_step = 3 # Number of steps the input is shifted to the right\n",
    "batch_size = 5\n",
    "n_batches = total_series_length//batch_size//truncated_backprop_steps\n",
    "\n",
    "eta = 0.01 # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "The input data of our recurrent network will be a vector of random numbers in the range (n_classes). For example, when setting \"n_classes = 2\", the input vector will contain random binary digits, e.g. [0,1,0,0,0,1,1,0,1,...].\n",
    "\n",
    "The target output is simply the input vector, shifted \"echo_steps\" to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    \"\"\"\n",
    "    Generates training data. The input data is simply a vector of random\n",
    "    numbers with n_classes classes. The target output is the input shifted \n",
    "    by \"echo_steps\" steps to the right.\n",
    "    \n",
    "    Returns:\n",
    "        x: numpy array of shape (batch_size,-1) filled with random values\n",
    "        in the range (n_classes)\n",
    "        \n",
    "        y: numpy array of shape (batch_size, -1), x shifted \"echo_step\" to \n",
    "        the right\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.array(np.random.choice(n_classes, total_series_length))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "\n",
    "The basic architecture of a recurrent network looks as follows ([source](http://www.deeplearningbook.org/)). \n",
    "\n",
    "![alt text](basic_rnn.png \"Title\")\n",
    "\n",
    "\n",
    "As visibile in the figure, the state $h^{(t)}$  of the network depends both on the input $x^{(t)}$ and the previous state $h^{(t-1)}$.\n",
    "It is computed as follows:\n",
    "\n",
    "$$ h^{(t)} = \\sigma(U x^{(t)} + W h^{(t-1)} + b) $$\n",
    "\n",
    "with $\\sigma$ beign the $\\tanh$ in our implementation.\n",
    "\n",
    "\n",
    "The output is computed as: \n",
    "\n",
    "$$ o^{(t)} = V h^{(t)} + c $$\n",
    "$$ \\hat{y}^{(t)} = \\text{softmax}(o^{(t)}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create placeholders for the input, target output and state of the network\n",
    "X_placeholder = tf.placeholder(dtype=tf.float32, shape=[batch_size, truncated_backprop_steps])\n",
    "y_placeholder = tf.placeholder(dtype=tf.int32, shape=[batch_size, truncated_backprop_steps])\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "\n",
    "# Create variables for weights and biases. To save computation, the two\n",
    "# weight matrices U and W are concatenated such that we have to perform only\n",
    "# one matrix multiplication when calculating the state of the recurrent network.\n",
    "U_W_concat = tf.Variable(np.random.randn(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "V = tf.Variable(np.random.randn(state_size, n_classes), dtype=tf.float32)\n",
    "c = tf.Variable(np.zeros((1,n_classes)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# We split the batch data into adjacent time steps by unpacking the columns of\n",
    "# the input batch into a list. This causes the network to be trained \n",
    "# simultaenously on multiple (namely \"batch_size\") parts of the input time \n",
    "# series. We account for this by setting the state of the network (\"init_state\")\n",
    "# to have \"batch_size\" rows.\n",
    "\n",
    "input_series = tf.unstack(X_placeholder, axis=1)\n",
    "labels_series = tf.unstack(y_placeholder, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_5:0' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:1' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:2' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:3' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:4' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:5' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:6' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:7' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:8' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:9' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:10' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:11' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:12' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:13' shape=(5,) dtype=int32>,\n",
       " <tf.Tensor 'unstack_5:14' shape=(5,) dtype=int32>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "To perform the forward pass we initialize the state of the RNN to init_state and then process all input examples one after the other.\n",
    "As mentioned before, the input $x^{(t)}$ and state $h^{(t-1)}$ of the network are concatenated such that we have to perform only a single matrix multiplication.\n",
    "\n",
    "All states are saved in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "current_state = init_state\n",
    "state_series = []\n",
    "\n",
    "for i in input_series:\n",
    "    inp = tf.reshape(i, [batch_size, 1])\n",
    "    inp_state_concat = tf.concat([inp, current_state],1)\n",
    "    new_state = tf.tanh(tf.matmul(inp_state_concat,U_W_concat)+b)\n",
    "    \n",
    "    state_series.append(new_state)\n",
    "    current_state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Network output and loss function\n",
    "\n",
    "To compute the output at each time step, we apply the softmax function as explained before.\n",
    "The loss is given by computing the cross entropy of the logits. The loss is minimized using the Adam Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Softmax output layer, compute output for each state\n",
    "logits = [tf.matmul(state, V)+c for state in state_series]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Compute loss\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for label, logit in zip(labels_series, logits)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "# Training step\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=eta).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train the network for the given number of epoch. For each epoch, we generate a new set of training examples that is fed into the network in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  0\n",
      "Step:  0 Loss: 0.935875\n",
      "Step:  100 Loss: 0.632052\n",
      "Step:  200 Loss: 0.637527\n",
      "Step:  300 Loss: 0.632971\n",
      "Step:  400 Loss: 0.55331\n",
      "Step:  500 Loss: 0.652074\n",
      "Step:  600 Loss: 0.515228\n",
      "\n",
      "Epoch:  1\n",
      "Step:  0 Loss: 0.50271\n",
      "Step:  100 Loss: 0.42879\n",
      "Step:  200 Loss: 0.227381\n",
      "Step:  300 Loss: 0.0520921\n",
      "Step:  400 Loss: 0.0391598\n",
      "Step:  500 Loss: 0.00913788\n",
      "Step:  600 Loss: 0.00560134\n",
      "\n",
      "Epoch:  2\n",
      "Step:  0 Loss: 0.174255\n",
      "Step:  100 Loss: 0.00338372\n",
      "Step:  200 Loss: 0.00313064\n",
      "Step:  300 Loss: 0.00219448\n",
      "Step:  400 Loss: 0.00193352\n",
      "Step:  500 Loss: 0.0016262\n",
      "Step:  600 Loss: 0.00136009\n",
      "\n",
      "Epoch:  3\n",
      "Step:  0 Loss: 0.21995\n",
      "Step:  100 Loss: 0.00122587\n",
      "Step:  200 Loss: 0.000853802\n",
      "Step:  300 Loss: 0.000786432\n",
      "Step:  400 Loss: 0.000666917\n",
      "Step:  500 Loss: 0.000540848\n",
      "Step:  600 Loss: 0.000626027\n",
      "\n",
      "Epoch:  4\n",
      "Step:  0 Loss: 0.242856\n",
      "Step:  100 Loss: 0.000642133\n",
      "Step:  200 Loss: 0.000426864\n",
      "Step:  300 Loss: 0.000351805\n",
      "Step:  400 Loss: 0.000398155\n",
      "Step:  500 Loss: 0.000401013\n",
      "Step:  600 Loss: 0.000326339\n",
      "\n",
      "Epoch:  5\n",
      "Step:  0 Loss: 0.165758\n",
      "Step:  100 Loss: 0.000538883\n",
      "Step:  200 Loss: 0.000351422\n",
      "Step:  300 Loss: 0.000232993\n",
      "Step:  400 Loss: 0.000267738\n",
      "Step:  500 Loss: 0.000221643\n",
      "Step:  600 Loss: 0.000161152\n",
      "\n",
      "Epoch:  6\n",
      "Step:  0 Loss: 0.198601\n",
      "Step:  100 Loss: 0.000487564\n",
      "Step:  200 Loss: 0.000334723\n",
      "Step:  300 Loss: 0.000164647\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d412176f9c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mX_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0my_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0minit_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_current_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             })\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carot/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carot/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carot/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carot/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carot/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        X_gen, y_gen = generateData()\n",
    "        \n",
    "        # The initial state of the network is set to zero\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Epoch: \", epoch)\n",
    "        \n",
    "        for batch_number in range(n_batches):\n",
    "            start_idx = batch_number * truncated_backprop_steps\n",
    "            end_idx = start_idx + truncated_backprop_steps\n",
    "            \n",
    "            batch_x = X_gen[:, start_idx:end_idx]\n",
    "            batch_y = y_gen[:, start_idx:end_idx]\n",
    "            \n",
    "            _total_loss, _train_step, _current_state = sess.run(\n",
    "            [total_loss, train_step, current_state], \n",
    "            feed_dict={\n",
    "                X_placeholder: batch_x,\n",
    "                y_placeholder: batch_y,\n",
    "                init_state: _current_state\n",
    "            })\n",
    "            \n",
    "            training_losses.append(_total_loss)\n",
    "            \n",
    "            if batch_number%100 == 0:\n",
    "                print(\"Step: \", batch_number, \"Loss:\", _total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8));\n",
    "plt.plot(training_losses);\n",
    "plt.xlabel('Number of training iterations');\n",
    "plt.ylabel('Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
